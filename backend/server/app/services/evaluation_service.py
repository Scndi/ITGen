import logging
import json
import os
import time
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime
from app.extensions import db

logger = logging.getLogger(__name__)

class EvaluationService:
    """è¯„ä¼°æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°æœåŠ¡"""
        self.reports = {}
        # æŸ¥æ‰¾ç»“æœæ–‡ä»¶ï¼šä¼˜å…ˆæŸ¥æ‰¾ result/ ç›®å½•ï¼Œç„¶åæ˜¯ä¸Šçº§ result/ ç›®å½•
        self.base_dir = Path(__file__).resolve().parent.parent.parent.parent
        # å°è¯•å¤šä¸ªå¯èƒ½çš„resultç›®å½•è·¯å¾„
        self.result_dirs = [
            self.base_dir / 'result',  # /home/king/project/ITGen/backend/result
            Path('/home/king/project/ITGen/backend/result'),  # ç»å¯¹è·¯å¾„
            self.base_dir.parent / 'result',  # /home/king/project/ITGen/result (å¤‡ç”¨)
        ]
        logger.info(f"ğŸ“ è¯„ä¼°æœåŠ¡åˆå§‹åŒ–ï¼Œç»“æœç›®å½•: {[str(d) for d in self.result_dirs]}")
       
    
    def get_all_reports(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æŠ¥å‘Š"""
        try:
            from app.models.db_evaluation import EvaluationReport
            reports = EvaluationReport.query.order_by(EvaluationReport.created_at.desc()).all()
            return [report.to_dict() for report in reports]
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æŠ¥å‘Šåˆ—è¡¨å¤±è´¥: {e}")
            # å¦‚æœæ•°æ®åº“å¤±è´¥ï¼Œä»å†…å­˜è¿”å›
            return list(self.reports.values())
    
    def get_report(self, report_id: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šæŠ¥å‘Š"""
        try:
            from app.models.db_evaluation import EvaluationReport
            report = EvaluationReport.query.filter_by(report_id=report_id).first()
            if report:
                return report.to_dict()
            else:
                # å¦‚æœæ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•ä»å†…å­˜è·å–
                return self.reports.get(report_id)
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æŠ¥å‘Šå¤±è´¥: {e}")
            # å¦‚æœæ•°æ®åº“å¤±è´¥ï¼Œä»å†…å­˜è¿”å›
            return self.reports.get(report_id)
    
    def generate_report_from_results(self, model_name: str, task_type: str, 
                                    attack_methods: List[str], 
                                    evaluation_metrics: List[str] = None) -> Dict[str, Any]:
        """
        ä»æ‰¹é‡æ”»å‡»ç»“æœæ–‡ä»¶ç”Ÿæˆé²æ£’æ€§è¯„ä¼°æŠ¥å‘Š
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'codebert', 'codet5'ï¼‰
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚ 'clone-detection'ï¼‰
            attack_methods: æ”»å‡»æ–¹æ³•åˆ—è¡¨ï¼ˆå¦‚ ['itgen', 'alert']ï¼‰
            evaluation_metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨ï¼ˆå¦‚ ['asr', 'ami', 'art']ï¼‰
        
        Returns:
            è¯„ä¼°æŠ¥å‘Šå­—å…¸
        """
        try:
            # é»˜è®¤è¯„ä¼°æŒ‡æ ‡
            if evaluation_metrics is None:
                evaluation_metrics = ['asr', 'ami', 'art']
            
            logger.info(f"å¼€å§‹ä¸ºæ¨¡å‹ {model_name} ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
            logger.info(f"ä»»åŠ¡ç±»å‹: {task_type}, æ”»å‡»æ–¹æ³•: {attack_methods}")
            
            # 1. ä¸ºæ¯ç§æ”»å‡»æ–¹æ³•åˆ†åˆ«æŸ¥æ‰¾å¯¹åº”çš„ç»“æœæ–‡ä»¶
            attack_results = {}
            

            for method in attack_methods:
                logger.info(f"ğŸ” æŸ¥æ‰¾æ”»å‡»æ–¹æ³• {method} çš„ç»“æœæ–‡ä»¶...")
                logger.info(f"   æ¨¡å‹: {model_name}, ä»»åŠ¡ç±»å‹: {task_type}")
                result_files = []
                
                # æ„å»ºæ–‡ä»¶åæ¨¡å¼
                file_patterns = [
                    f"*{model_name}*{task_type}*{method}*.jsonl",
                    f"{model_name}_{task_type}_{method}*.jsonl",
                    f"attack_{model_name}*{method}*.jsonl",
                    f"attack_{model_name}_{task_type}*{method}*.jsonl",
                    f"*{method}*{model_name}*{task_type}*.jsonl"
                ]
                
                logger.info(f"   æ–‡ä»¶åŒ¹é…æ¨¡å¼: {file_patterns[:2]}...")
                
                # åœ¨æ‰€æœ‰ç»“æœç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶
                for result_dir in self.result_dirs:
                    if not result_dir.exists():
                        logger.warning(f"âš ï¸ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
                        continue
                    
                    logger.info(f"   åœ¨ç›®å½• {result_dir} ä¸­æŸ¥æ‰¾...")
                    for pattern in file_patterns:
                        files = list(result_dir.glob(pattern))
                        if files:
                            logger.info(f"   æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶: {[f.name for f in files]}")
                        result_files.extend(files)
                
                # å»é‡
                result_files = list(set(result_files))
                
                if not result_files:
                    logger.error(f"âŒ æœªæ‰¾åˆ°æ”»å‡»æ–¹æ³• {method} çš„ç»“æœæ–‡ä»¶")
                    logger.error(f"   æŸ¥æ‰¾çš„ç›®å½•: {[str(d) for d in self.result_dirs if d.exists()]}")
                    logger.error(f"   æŸ¥æ‰¾çš„æ¨¡å¼: {file_patterns}")
                    attack_results[method] = {
                        'files': [],
                        'all_results': [],
                        'successful_results': [],
                        'failed_results': []
                    }
                    continue
                
                logger.info(f"æ”»å‡»æ–¹æ³• {method} æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶: {[f.name for f in result_files]}")
                # 2. è¯»å–å¹¶å¤„ç†è¯¥æ”»å‡»æ–¹æ³•çš„ç»“æœ
                all_results = []
                for file_path in result_files:
                    logger.info(f"å¤„ç†æ–‡ä»¶: {file_path.name}")
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            for line_num, line in enumerate(f, 1):
                                if line.strip():
                                    try:
                                        result = json.loads(line.strip())
                                        # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
                                        result['_file_source'] = file_path.name
                                        all_results.append(result)
                                    except json.JSONDecodeError as e:
                                        logger.warning(f"æ–‡ä»¶ {file_path.name} ç¬¬ {line_num} è¡Œè§£æJSONå¤±è´¥: {e}")
                    except Exception as e:
                        logger.error(f"è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")
                
                # 3. æ ¹æ®Typeå­—æ®µåˆ†ç±»ç»“æœ
                # Type="0"ä»£è¡¨æ”»å‡»å¤±è´¥
                # Type!="0"ä»£è¡¨æ”»å‡»æˆåŠŸï¼ˆå¯èƒ½æ˜¯"Greedy", "itgen", "alert"ç­‰ï¼Œå–å†³äºæ”»å‡»æ–¹æ³•ï¼‰
                # å¯¹äºalertæ–¹æ³•ï¼ŒTypeå¯èƒ½æ˜¯"Greedy"ï¼›å¯¹äºitgenæ–¹æ³•ï¼ŒTypeå¯èƒ½æ˜¯"itgen"æˆ–å…¶ä»–å€¼
                # åªè¦Typeä¸æ˜¯"0"ï¼Œå°±è®¤ä¸ºæ˜¯æˆåŠŸçš„æ”»å‡»
                successful_results = [r for r in all_results if r.get('Type') != '0' and r.get('Type') is not None]
                failed_results = [r for r in all_results if r.get('Type') == '0']
                
                logger.info(f"   æ”»å‡»æ–¹æ³• {method} ç»“æœç»Ÿè®¡: æˆåŠŸ={len(successful_results)}, å¤±è´¥={len(failed_results)}, æ€»è®¡={len(all_results)}")
                if successful_results:
                    logger.info(f"   æˆåŠŸç»“æœçš„Typeå€¼ç¤ºä¾‹: {set(r.get('Type') for r in successful_results[:5])}")
                
                attack_results[method] = {
                    'files': result_files,
                    'all_results': all_results,
                    'successful_results': successful_results,
                    'failed_results': failed_results
                }
            
            # 4. åˆ†åˆ«ç»Ÿè®¡å„ä¸ªæ”»å‡»æ–¹æ³•çš„æŒ‡æ ‡
            method_metrics = {}
            overall_stats = {
                'total_samples': 0,
                'successful_attacks': 0,
                'failed_attacks': 0
            }
            
            for method in attack_methods:
                method_data = attack_results[method]
                all_results = method_data['all_results']
                successful_results = method_data['successful_results']
                failed_results = method_data['failed_results']
                
                total_samples = len(all_results)
                successful_attacks = len(successful_results)
                failed_attacks = len(failed_results)
                
                # æ›´æ–°æ€»ä½“ç»Ÿè®¡
                overall_stats['total_samples'] += total_samples
                overall_stats['successful_attacks'] += successful_attacks
                overall_stats['failed_attacks'] += failed_attacks
                
                # è®¡ç®—è¯¥æ”»å‡»æ–¹æ³•çš„å„é¡¹æŒ‡æ ‡
                # ASR - Attack Success Rate (æ”»å‡»æˆåŠŸç‡)
                asr = successful_attacks / total_samples if total_samples > 0 else 0
                
                # AMI - Average Model Invocations (å¹³å‡æ¨¡å‹è°ƒç”¨æ¬¡æ•°)
                if successful_attacks > 0:
                    ami = sum(r.get('Query Times', 0) for r in successful_results) / successful_attacks
                else:
                    ami = 0
                
                # ART - Average Response Time (å¹³å‡å“åº”æ—¶é—´)
                if successful_attacks > 0:
                    art = sum(r.get('Time Cost', 0) for r in successful_results) / successful_attacks
                else:
                    art = 0
                
                # å¹³å‡ä»£ç é•¿åº¦
                avg_program_length = sum(r.get('Program Length', 0) for r in all_results) / total_samples if total_samples > 0 else 0
                
                # å¹³å‡æ ‡è¯†ç¬¦æ•°é‡
                avg_identifiers = sum(r.get('Identifier Num', 0) for r in all_results) / total_samples if total_samples > 0 else 0
                
                method_metrics[method] = {
                    'total_samples': total_samples,
                    'successful_attacks': successful_attacks,
                    'failed_attacks': failed_attacks,
                    'asr': round(asr * 100, 2),  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                    'ami': round(ami, 2),
                    'art': round(art, 2),
                    'avg_program_length': round(avg_program_length, 2),
                    'avg_identifiers': round(avg_identifiers, 2),
                }
                
                logger.info(f"æ”»å‡»æ–¹æ³• {method} ç»Ÿè®¡: ASR={method_metrics[method]['asr']}%, "
                        f"æˆåŠŸ={successful_attacks}/{total_samples}")
            
            # 5. è®¡ç®—æ€»ä½“æŒ‡æ ‡
            total_samples = overall_stats['total_samples']
            successful_attacks = overall_stats['successful_attacks']
            failed_attacks = overall_stats['failed_attacks']
            
            logger.info(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡: æ€»æ ·æœ¬={total_samples}, æˆåŠŸ={successful_attacks}, å¤±è´¥={failed_attacks}")
            
            # å¦‚æœæ‰€æœ‰æ”»å‡»æ–¹æ³•éƒ½æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œè¿”å›é”™è¯¯
            if total_samples == 0:
                error_msg = f"æ‰€æœ‰æ”»å‡»æ–¹æ³•éƒ½æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶ã€‚æŸ¥æ‰¾çš„ç›®å½•: {[str(d) for d in self.result_dirs if d.exists()]}"
                logger.error(f"âŒ {error_msg}")
                return {
                    'success': False,
                    'error': error_msg
                }
            
            # æ€»ä½“ASR
            overall_asr = successful_attacks / total_samples if total_samples > 0 else 0
            
            # æ€»ä½“AMIå’ŒARTï¼ˆåŠ æƒå¹³å‡ï¼‰
            overall_ami = 0
            overall_art = 0
            overall_avg_program_length = 0
            overall_avg_identifiers = 0
            
            for method in attack_methods:
                method_data = method_metrics[method]
                weight = method_data['total_samples'] / total_samples if total_samples > 0 else 0
                
                overall_ami += method_data['ami'] * weight
                overall_art += method_data['art'] * weight
                overall_avg_program_length += method_data['avg_program_length'] * weight
                overall_avg_identifiers += method_data['avg_identifiers'] * weight
            
            # 6. ç”ŸæˆæŠ¥å‘ŠIDå’Œå®Œæ•´æŠ¥å‘Š
            report_id = f"{model_name}_{task_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # æ±‡æ€»ç»Ÿè®¡
            summary_stats = {
                'total_samples': total_samples,
                'successful_attacks': successful_attacks,
                'failed_attacks': failed_attacks,
                'asr': round(overall_asr * 100, 2),
                'ami': round(overall_ami, 2),
                'art': round(overall_art, 2),
                'avg_program_length': round(overall_avg_program_length, 2),
                'avg_identifiers': round(overall_avg_identifiers, 2)
            }
            
            # æ”¶é›†æ‰€æœ‰æˆåŠŸç»“æœçš„æ ·æœ¬ï¼ˆç”¨äºå±•ç¤ºï¼‰
            all_successful_results = []
            for method in attack_methods:
                all_successful_results.extend(attack_results[method]['successful_results'])
            
            report = {
                'report_id': report_id,
                'model_name': model_name,
                'task_type': task_type,
                'attack_methods': attack_methods,
                'evaluation_metrics': evaluation_metrics,
                'method_metrics': method_metrics,
                'summary_stats': summary_stats,
                'sample_results': all_successful_results[:5] if len(all_successful_results) > 5 else all_successful_results,
                'generated_at': datetime.now().isoformat()
            }
            
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸ: æ€»ä½“ASR={summary_stats['asr']}%, "
                    f"æ€»æˆåŠŸæ”»å‡»={successful_attacks}/{total_samples}")
            
            # 7. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœæ•°æ®åº“å¯ç”¨ï¼‰
            try:
                from app.models.db_evaluation import EvaluationReport
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæŠ¥å‘Š
                existing_report = EvaluationReport.query.filter_by(report_id=report_id).first()
                if existing_report:
                    # æ›´æ–°ç°æœ‰æŠ¥å‘Š
                    existing_report.asr = summary_stats['asr']
                    existing_report.ami = summary_stats['ami']
                    existing_report.art = summary_stats['art']
                    existing_report.total_samples = total_samples
                    existing_report.successful_attacks = successful_attacks
                    existing_report.failed_attacks = failed_attacks
                    existing_report.avg_program_length = summary_stats['avg_program_length']
                    existing_report.avg_identifiers = summary_stats['avg_identifiers']
                    existing_report.method_metrics = method_metrics
                    existing_report.summary_stats = summary_stats
                    existing_report.sample_results = report['sample_results']
                    db.session.commit()
                    logger.info(f"æ›´æ–°ç°æœ‰æŠ¥å‘Š: {report_id}")
                else:
                    # åˆ›å»ºæ–°æŠ¥å‘Š
                    evaluation_report = EvaluationReport(
                        report_id=report_id,
                        model_name=model_name,
                        task_type=task_type,
                        attack_methods=attack_methods,
                        evaluation_metrics=evaluation_metrics,
                        total_samples=total_samples,
                        successful_attacks=successful_attacks,
                        failed_attacks=failed_attacks,
                        asr=summary_stats['asr'],
                        ami=summary_stats['ami'],
                        art=summary_stats['art'],
                        avg_program_length=summary_stats['avg_program_length'],
                        avg_identifiers=summary_stats['avg_identifiers'],
                        method_metrics=method_metrics,
                        summary_stats=summary_stats,
                        sample_results=report['sample_results']
                    )
                    db.session.add(evaluation_report)
                    db.session.commit()
                    logger.info(f"ä¿å­˜æ–°æŠ¥å‘Šåˆ°æ•°æ®åº“: {report_id}")
                
                # åŒæ—¶ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
                self.reports[report_id] = report
                
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“å¤±è´¥ï¼ˆä¸å½±å“ç»“æœè¿”å›ï¼‰: {e}")
                logger.debug(f"æ•°æ®åº“é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {e}", exc_info=True)
                # å³ä½¿æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œä¹Ÿè¿”å›æŠ¥å‘Šç»“æœ
                self.reports[report_id] = report
            
            logger.info(f"âœ… è¯„ä¼°æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {report_id}")
            logger.info(f"   æ€»æ ·æœ¬æ•°: {total_samples}, æˆåŠŸæ”»å‡»: {successful_attacks}, å¤±è´¥æ”»å‡»: {failed_attacks}")
            logger.info(f"   æ€»ä½“ASR: {summary_stats['asr']}%, AMI: {summary_stats['ami']}, ART: {summary_stats['art']}")
            
            return {
                'success': True,
                'report_id': report_id,
                'report': report
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}", exc_info=True)
            import traceback
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            return {
                'success': False,
                'error': str(e)
            }