import sys
import os

# ========== é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥ä»»ä½• huggingface ç›¸å…³æ¨¡å—ä¹‹å‰è®¾ç½®é•œåƒç«™ ==========
# é…ç½® Hugging Face é•œåƒç«™ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ huggingface_hub æˆ– transformers ä¹‹å‰è®¾ç½®ï¼‰
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é•œåƒç«™
if 'HF_ENDPOINT' not in os.environ:
    # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæ— æ³•å¯¼å…¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
    try:
        # å»¶è¿Ÿå¯¼å…¥ Configï¼Œé¿å…å¾ªç¯ä¾èµ–
        import importlib.util
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.py')
        spec = importlib.util.spec_from_file_location("config", config_path)
        if spec and spec.loader:
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            hf_endpoint = getattr(config_module.Config, 'HF_ENDPOINT', 'https://hf-mirror.com')
        else:
            hf_endpoint = 'https://hf-mirror.com'
    except Exception:
        hf_endpoint = 'https://hf-mirror.com'
    
    os.environ['HF_ENDPOINT'] = hf_endpoint
    # åŒæ—¶è®¾ç½® HF_HUB_ENDPOINTï¼ˆæŸäº›ç‰ˆæœ¬å¯èƒ½éœ€è¦ï¼‰
    os.environ['HF_HUB_ENDPOINT'] = hf_endpoint
else:
    hf_endpoint = os.environ['HF_ENDPOINT']
# ========== é•œåƒç«™é…ç½®ç»“æŸ ==========

import json
import random
import re
import torch
import numpy as np
from pathlib import Path
import time
import logging
from typing import Dict, Any, List

# å¯¼å…¥è„šæœ¬æ‰§è¡ŒæœåŠ¡
from app.services.script_execution_service import ScriptExecutionService
from app.config import Config
from app.utils.device import get_device_from_config
from app.models.db_models import Model as DBModel
from pathlib import Path

logger = logging.getLogger(__name__)

# å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰ä¸åŒçš„è®¾ç½®ï¼Œä¸”ä¹‹å‰ä½¿ç”¨çš„æ˜¯é»˜è®¤å€¼ï¼Œåˆ™æ›´æ–°ä¸ºé…ç½®æ–‡ä»¶çš„å€¼
# æ³¨æ„ï¼šæ­¤æ—¶ transformers å¯èƒ½å·²ç»å¯¼å…¥ï¼Œä½†ç¯å¢ƒå˜é‡ä»ä¼šå½±å“åç»­çš„æ¨¡å‹ä¸‹è½½
if hasattr(Config, 'HF_ENDPOINT') and Config.HF_ENDPOINT != hf_endpoint:
    os.environ['HF_ENDPOINT'] = Config.HF_ENDPOINT
    os.environ['HF_HUB_ENDPOINT'] = Config.HF_ENDPOINT
    hf_endpoint = Config.HF_ENDPOINT
    logger.info(f"âœ“ æ›´æ–° Hugging Face é•œåƒç«™ä¸ºé…ç½®æ–‡ä»¶ä¸­çš„å€¼: {hf_endpoint}")
else:
    logger.info(f"âœ“ Hugging Face é•œåƒç«™: {hf_endpoint}")

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
BASE_DIR = Path(__file__).resolve().parent.parent.parent.parent
sys.path.append(str(BASE_DIR))

# æ”¯æŒçš„ä»»åŠ¡ç±»å‹è·¯å¾„
SUPPORTED_TASK_TYPES = [
    'clone-detection',
    'code-summarization',
    'vulnerability-prediction',
    'authorship-attribution',
    'vulnerability-detection'
]

# ä¸ºæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡ç±»å‹æ·»åŠ è·¯å¾„
for task_type in SUPPORTED_TASK_TYPES:
    task_code_path = BASE_DIR / 'roberta' / task_type / 'code'
    if task_code_path.exists():
        sys.path.append(str(task_code_path))

sys.path.append(str(BASE_DIR / 'python_parser'))

# å¯¼å…¥ITGenç›¸å…³æ¨¡å—
# ITGenç°åœ¨é€šè¿‡ç»Ÿä¸€æ¥å£ä½¿ç”¨ï¼Œä¸å†éœ€è¦ç›´æ¥å¯¼å…¥


class AttackService:
    """æ”»å‡»æœåŠ¡ç±» - ç»Ÿä¸€æ”»å‡»æ¥å£ï¼Œæ”¯æŒå¤šç§æ”»å‡»æ–¹æ³•"""

    def __init__(self):
        """åˆå§‹åŒ–æ”»å‡»æœåŠ¡"""
        self.models = {}  # æ¨¡å‹ç¼“å­˜
        self.attackers = {}  # ç»Ÿä¸€æ”»å‡»å™¨ç¼“å­˜
        self.mlm_model = None  # MLMæ¨¡å‹ç¼“å­˜
        self.mlm_tokenizer = None  # MLM tokenizerç¼“å­˜
        self.id2token_cache = None  # id2tokenç¼“å­˜
        self.script_executor = ScriptExecutionService()  # è„šæœ¬æ‰§è¡Œå™¨ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
        
    def _load_model(self, model_name='codebert', model_id: int = None, task_type: str = 'clone-detection'):
        """
        åŠ è½½æ¨¡å‹ï¼ˆä»æ•°æ®åº“æˆ–ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰

        Args:
            model_name: æ¨¡å‹åç§°
            model_id: æ¨¡å‹IDï¼ˆå¦‚æœæä¾›ï¼Œä»æ•°æ®åº“åŠ è½½ï¼‰
            task_type: ä»»åŠ¡ç±»å‹ (clone-detection, code-summarization, vulnerability-prediction, authorship-attribution, vulnerability-detection)
        """
        # éªŒè¯ä»»åŠ¡ç±»å‹
        if task_type not in SUPPORTED_TASK_TYPES:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}ã€‚æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {SUPPORTED_TASK_TYPES}")
        # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å…ç½‘ç»œé—®é¢˜
        cache_dir = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))
        
        # ä»æ•°æ®åº“åŠ è½½æ¨¡å‹ä¿¡æ¯
        model_path = None
        tokenizer_path = None
        checkpoint_path = None
        mlm_model_path = None
        
        # å¦‚æœæä¾›äº†model_idï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™é€šè¿‡model_nameæŸ¥æ‰¾
        db_model = None
        if model_id:
            db_model = DBModel.query.filter_by(id=model_id).first()
        elif model_name:
            # é€šè¿‡model_nameæŸ¥æ‰¾æ¨¡å‹
            db_model = DBModel.query.filter_by(model_name=model_name).first()
            if db_model:
                model_id = db_model.id
        
        if db_model:
            model_path = db_model.model_path
            tokenizer_path = db_model.tokenizer_path
            checkpoint_path = db_model.checkpoint_path
            mlm_model_path = db_model.mlm_model_path
            logger.info(f"âœ“ ä»æ•°æ®åº“åŠ è½½æ¨¡å‹ä¿¡æ¯: {db_model.model_name} (ID: {db_model.id})")
        else:
            if model_id:
                logger.warning(f"âš  æ¨¡å‹ID {model_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            elif model_name:
                logger.warning(f"âš  æ¨¡å‹åç§° {model_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # å¦‚æœæ²¡æœ‰ä»æ•°æ®åº“è·å–ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        if not model_path:
            model_path = 'microsoft/codebert-base'
        if not tokenizer_path:
            tokenizer_path = 'microsoft/codebert-base'
        
        # åˆ¤æ–­æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯HuggingFaceè·¯å¾„
        def is_local_path(path: str) -> bool:
            """åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„"""
            return os.path.exists(path) or Path(path).is_absolute() or not '/' in path or path.startswith('./') or path.startswith('../')
        
        try:
            # åŠ è½½tokenizer
            if is_local_path(tokenizer_path):
                tokenizer = RobertaTokenizer.from_pretrained(
                    tokenizer_path,
                    local_files_only=True
                )
                logger.info(f"âœ“ Tokenizerä»æœ¬åœ°åŠ è½½: {tokenizer_path}")
            else:
                tokenizer = RobertaTokenizer.from_pretrained(
                    tokenizer_path,
                    cache_dir=cache_dir
                )
                logger.info(f"âœ“ Tokenizerä»HuggingFaceåŠ è½½: {tokenizer_path}")
            
            # åŠ è½½é…ç½®
            if is_local_path(model_path):
                config = RobertaConfig.from_pretrained(
                    model_path,
                    local_files_only=True
                )
                logger.info(f"âœ“ é…ç½®ä»æœ¬åœ°åŠ è½½: {model_path}")
            else:
                config = RobertaConfig.from_pretrained(
                    model_path,
                    cache_dir=cache_dir
                )
                logger.info(f"âœ“ é…ç½®ä»HuggingFaceåŠ è½½: {model_path}")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®æ ‡ç­¾æ•°é‡
            if task_type in ['clone-detection', 'vulnerability-prediction', 'vulnerability-detection']:
                config.num_labels = 2  # äºŒåˆ†ç±»ä»»åŠ¡
            elif task_type == 'authorship-attribution':
                config.num_labels = 10  # å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆæ ¹æ®æ•°æ®é›†è°ƒæ•´ï¼‰
            elif task_type == 'code-summarization':
                config.num_labels = 1  # ç”Ÿæˆä»»åŠ¡
            else:
                config.num_labels = 2  # é»˜è®¤äºŒåˆ†ç±»

            logger.info(f"âœ“ ä»»åŠ¡ç±»å‹: {task_type}, æ ‡ç­¾æ•°é‡: {config.num_labels}")
            
            # åŠ è½½æ¨¡å‹
            if is_local_path(model_path):
                encoder = RobertaModel.from_pretrained(
                    model_path,
                    local_files_only=True
                )
                logger.info(f"âœ“ æ¨¡å‹ç¼–ç å™¨ä»æœ¬åœ°åŠ è½½: {model_path}")
            else:
                encoder = RobertaModel.from_pretrained(
                    model_path,
                    cache_dir=cache_dir
                )
                logger.info(f"âœ“ æ¨¡å‹ç¼–ç å™¨ä»HuggingFaceåŠ è½½: {model_path}")
            
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
        
        # è·å–è®¡ç®—è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼Œæ‰¾ä¸åˆ°åˆ™CPUï¼‰
        device = get_device_from_config(Config)
        args = type('args', (), {
            'block_size': 512,
            'device': device,
            'model_name': model_name,
            'eval_batch_size': 4,
            'tokenizer': tokenizer,
            'language': 'java'
        })()
        
        model = Model(encoder, config, tokenizer, args)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼ˆæ£€æŸ¥ç‚¹ï¼‰
        if checkpoint_path and Path(checkpoint_path).exists():
            try:
                model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)
                logger.info(f"âœ“ åŠ è½½å¾®è°ƒæƒé‡: {checkpoint_path}")
            except Exception as e:
                logger.warning(f"âš  åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}, ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        else:
            # æ ¹æ®ä»»åŠ¡ç±»å‹å°è¯•åŠ è½½é»˜è®¤æ£€æŸ¥ç‚¹
            default_checkpoint = None

            # å°è¯•å¤šä¸ªå¯èƒ½çš„æ£€æŸ¥ç‚¹è·¯å¾„
            possible_paths = [
                BASE_DIR / model_name / task_type / 'saved_models' / 'checkpoint-best-f1' / f'{model_name}_model.bin',
                BASE_DIR / model_name / task_type / 'saved_models' / 'checkpoint-best-f1' / 'pytorch_model.bin',
                BASE_DIR / 'saved_models' / model_name / task_type / 'checkpoint-best-f1' / f'{model_name}_model.bin',
                BASE_DIR / 'CodeBERT' / task_type / 'saved_models' / 'checkpoint-best-f1' / 'codebert_model.bin',  # å‘åå…¼å®¹
            ]

            for checkpoint_path in possible_paths:
                if checkpoint_path.exists():
                    default_checkpoint = checkpoint_path
                    break

            if default_checkpoint:
                try:
                    model.load_state_dict(torch.load(default_checkpoint, map_location=device), strict=False)
                    logger.info(f"âœ“ åŠ è½½é»˜è®¤å¾®è°ƒæƒé‡: {default_checkpoint}")
                except Exception as e:
                    logger.warning(f"âš  åŠ è½½é»˜è®¤æƒé‡å¤±è´¥: {e}")
            else:
                logger.info("â„¹ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        
        # ç§»åŠ¨åˆ°GPU
        model.to(device)
        model.eval()
        logger.info(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°: {device}")
        
        # ç¼“å­˜æ¨¡å‹
        self.models[model_name] = {
            'model': model,
            'tokenizer': tokenizer,
            'config': config,
            'args': args
        }
        
        return self.models[model_name]
    
    def _create_attacker(self, model_name='codebert', model_id: int = None, task_type: str = 'clone-detection'):
        """
        åˆ›å»ºæ”»å‡»å™¨ï¼ˆæ—§æ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
        """
        logger.warning("âš  ä½¿ç”¨æ—§çš„æ”»å‡»å™¨åˆ›å»ºæ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨_create_unified_attacker")
        return self._create_unified_attacker('itgen', None, None, {'model_id': model_id, 'task_type': task_type})

    def _create_unified_attacker(self, method: str, model=None, tokenizer=None, config: Dict[str, Any] = None):
        """
        åˆ›å»ºç»Ÿä¸€æ”»å‡»å™¨

        Args:
            method: æ”»å‡»æ–¹æ³• ('itgen', 'beam', 'alert', etc.)
            model: æ¨¡å‹å®ä¾‹ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šä»configä¸­è·å–ï¼‰
            tokenizer: tokenizerå®ä¾‹ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šä»configä¸­è·å–ï¼‰
            config: é…ç½®å‚æ•°

        Returns:
            ç»Ÿä¸€æ”»å‡»å™¨å®ä¾‹
        """
        if config is None:
            config = {}

        model_id = config.get('model_id')
        model_name = config.get('model_name', 'codebert')
        task_type = config.get('task_type', 'clone-detection')

        # å¦‚æœæ²¡æœ‰æä¾›modelå’Œtokenizerï¼Œéœ€è¦åŠ è½½
        if model is None or tokenizer is None:
            model_data = self._load_model(model_name, model_id=model_id, task_type=task_type)
            model = model_data['model']
            tokenizer = model_data['tokenizer']

        # åˆ›å»ºç¼“å­˜key
        cache_key = f"{method}_{model_name}_{model_id}" if model_id else f"{method}_{model_name}"
        if cache_key in self.attackers:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ”»å‡»å™¨: {cache_key}")
            return self.attackers[cache_key]

        logger.info(f"åˆ›å»ºç»Ÿä¸€æ”»å‡»å™¨: {method}")

        try:
            # è·å–æ”»å‡»å™¨ç±»
            from app.attacks import get_attacker_class
            attacker_class = get_attacker_class(method)

            # åˆ›å»ºæ”»å‡»å™¨
            attacker = attacker_class(model, tokenizer, config)
            self.attackers[cache_key] = attacker
            logger.info(f"âœ“ {method.upper()}æ”»å‡»å™¨åˆ›å»ºæˆåŠŸ")

            return attacker

        except Exception as e:
            logger.error(f"âœ— åˆ›å»º{method.upper()}æ”»å‡»å™¨å¤±è´¥: {e}")
            raise
    
    def _load_mlm_model(self, base_model='microsoft/codebert-base-mlm', model_id: int = None, model_name: str = None):
        """
        åŠ è½½CodeBERT MLMæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            base_model: é»˜è®¤MLMæ¨¡å‹è·¯å¾„
            model_id: æ¨¡å‹IDï¼ˆå¦‚æœæä¾›ï¼Œä»æ•°æ®åº“åŠ è½½MLMæ¨¡å‹è·¯å¾„ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚æœæä¾›ä¸”model_idä¸ºNoneï¼Œé€šè¿‡åç§°æŸ¥æ‰¾ï¼‰
        """
        if self.mlm_model is not None and model_id is None and model_name is None:
            logger.debug("ä½¿ç”¨ç¼“å­˜çš„MLMæ¨¡å‹")
            return self.mlm_model, self.mlm_tokenizer
        
        # ä»æ•°æ®åº“åŠ è½½MLMæ¨¡å‹è·¯å¾„
        mlm_model_path = base_model
        db_model = None
        if model_id:
            db_model = DBModel.query.filter_by(id=model_id).first()
        elif model_name:
            db_model = DBModel.query.filter_by(model_name=model_name).first()
        
        if db_model and db_model.mlm_model_path:
            mlm_model_path = db_model.mlm_model_path
            logger.info(f"âœ“ ä»æ•°æ®åº“è·å–MLMæ¨¡å‹è·¯å¾„: {mlm_model_path}")
        
        logger.info(f"åŠ è½½MLMæ¨¡å‹: {mlm_model_path}")
        try:
            from transformers import RobertaForMaskedLM, RobertaTokenizer
            
            # åˆ¤æ–­æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯HuggingFaceè·¯å¾„
            def is_local_path(path: str) -> bool:
                return os.path.exists(path) or Path(path).is_absolute() or not '/' in path or path.startswith('./') or path.startswith('../')
            
            cache_dir = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))
            
            # åŠ è½½tokenizer
            if is_local_path(mlm_model_path):
                tokenizer = RobertaTokenizer.from_pretrained(
                    mlm_model_path,
                    local_files_only=True
                )
                logger.info(f"âœ“ MLM Tokenizerä»æœ¬åœ°åŠ è½½: {mlm_model_path}")
            else:
                tokenizer = RobertaTokenizer.from_pretrained(
                    mlm_model_path,
                    cache_dir=cache_dir
                )
                logger.info(f"âœ“ MLM Tokenizerä»HuggingFaceåŠ è½½: {mlm_model_path}")
            
            # åŠ è½½MLMæ¨¡å‹
            if is_local_path(mlm_model_path):
                model = RobertaForMaskedLM.from_pretrained(
                    mlm_model_path,
                    local_files_only=True
                )
                logger.info(f"âœ“ MLMæ¨¡å‹ä»æœ¬åœ°åŠ è½½: {mlm_model_path}")
            else:
                model = RobertaForMaskedLM.from_pretrained(
                    mlm_model_path,
                    cache_dir=cache_dir
                )
                logger.info(f"âœ“ MLMæ¨¡å‹ä»HuggingFaceåŠ è½½: {mlm_model_path}")
            
            # è·å–è®¡ç®—è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼Œæ‰¾ä¸åˆ°åˆ™CPUï¼‰
            device = get_device_from_config(Config)
            model.to(device)
            model.eval()
            
            logger.info(f"âœ“ MLMæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
            
            self.mlm_model = model
            self.mlm_tokenizer = tokenizer
            
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âœ— åŠ è½½MLMæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def build_id2token_from_code(self, code_data, language='java', vocab_size=5000):
        """
        ä»è¾“å…¥ä»£ç ä¸­æå–æ ‡è¯†ç¬¦æ„å»ºè¯æ±‡åº“ï¼ˆid2tokenï¼‰
        
        Args:
            code_data: ä»£ç æ•°æ®å­—å…¸ï¼ŒåŒ…å«code1å’Œcode2
            language: ç¼–ç¨‹è¯­è¨€
            vocab_size: è¯æ±‡åº“å¤§å°é™åˆ¶
        
        Returns:
            id2token: è¯æ±‡åˆ—è¡¨
            token2idx: è¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„
        """
        logger.info(f"ğŸ”¤ ä»ä»£ç ä¸­æå–æ ‡è¯†ç¬¦æ„å»ºè¯æ±‡åº“ï¼ˆæœ€å¤š{vocab_size}ä¸ªï¼‰...")
        
        try:
            from utils import build_vocab
            
            code_tokens = []
            processed_count = 0
            
            # for idx, code_data in enumerate(code_data_list):
            #     if not isinstance(code_data, dict):
            #         continue
                    
            code1 = code_data.get('code1', '')
            code2 = code_data.get('code2', '')
                
            # æå–code1çš„æ ‡è¯†ç¬¦
            try:
                identifiers, tokens = get_identifiers(code1, language)
                code_tokens.append(tokens)
                processed_count += 1
                logger.debug(f"âœ“ ä»code1æå–äº† {len(tokens)} ä¸ªtoken")
            except Exception as e:
                logger.warning(f"âš  æå–code1æ ‡è¯†ç¬¦å¤±è´¥: {e}")
            
            # æå–code2çš„æ ‡è¯†ç¬¦
            if code2:
                try:
                    identifiers, tokens = get_identifiers(code2, language)
                    code_tokens.append(tokens)
                    processed_count += 1
                    logger.debug(f"âœ“ ä»code2æå–äº† {len(tokens)} ä¸ªtoken")
                except Exception as e:
                    logger.warning(f"âš  æå–code2æ ‡è¯†ç¬¦å¤±è´¥: {e}")
        
            if len(code_tokens) == 0:
                logger.error("âœ— æœªèƒ½æå–ä»»ä½•æ ‡è¯†ç¬¦")
                return [], {}
            
            # æ„å»ºè¯æ±‡åº“
            id2token, token2idx = build_vocab(code_tokens, vocab_size)
            
            logger.info(f"âœ“ æˆåŠŸå¤„ç† {processed_count} æ®µä»£ç ")
            logger.info(f"âœ“ è¯æ±‡åº“å¤§å°: {len(id2token)} ä¸ªæ ‡è¯†ç¬¦")
            logger.debug(f"  ç¤ºä¾‹è¯æ±‡ï¼ˆå‰10ä¸ªï¼‰: {id2token[:10]}")
            
            # ç¼“å­˜ç»“æœ
            self.id2token_cache = id2token
            
            return id2token, token2idx
            
        except Exception as e:
            logger.error(f"âœ— æ„å»ºid2tokenå¤±è´¥: {e}", exc_info=True)
            return [], {}
    
    def sample_random_substitutes(self, code, substitutes, id2token, num_random_per_key=50):
        """
        ä¸ºæ¯ä¸ªå˜é‡é‡‡æ ·éšæœºæ›¿æ¢è¯ï¼ˆæ¨¡æ‹Ÿattack_itgen.pyçš„é€»è¾‘ï¼‰
        
        Args:
            code: åŸå§‹ä»£ç 
            substitutes: åŸå§‹æ›¿æ¢è¯å­—å…¸ {identifier: [candidates]}
            id2token: è¯æ±‡åº“åˆ—è¡¨
            num_random_per_key: æ¯ä¸ªå˜é‡åˆ†é…å¤šå°‘ä¸ªéšæœºè¯
        
        Returns:
            sampled_substitutes: é‡‡æ ·åçš„æ›¿æ¢è¯å­—å…¸
        """
        import re
        
        if not id2token:
            logger.warning("âš  id2tokenä¸ºç©ºï¼Œè¿”å›åŸå§‹æ›¿æ¢è¯")
            return substitutes
        
        logger.info("ğŸ² é‡‡æ ·éšæœºæ›¿æ¢è¯...")
        
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æœ‰æ•ˆæ ‡è¯†ç¬¦
        uid_pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')
        
        # è®¡ç®—éœ€è¦çš„æ€»è¯æ•°
        total_needed = len(substitutes.keys()) * num_random_per_key
        
        if len(id2token) < total_needed:
            logger.warning(f"âš  è¯æ±‡åº“({len(id2token)})ä¸è¶³ä»¥é‡‡æ ·{total_needed}ä¸ªè¯ï¼Œä½¿ç”¨å…¨éƒ¨è¯æ±‡")
            total_needed = len(id2token)
        
        # éšæœºé‡‡æ ·
        selected_tmp_sub = random.sample(id2token, min(total_needed, len(id2token)))
        
        # åˆ†ç»„ï¼šæ¯ä¸ªå˜é‡åˆ†é…num_random_per_keyä¸ªè¯
        sublists = [selected_tmp_sub[i:i+num_random_per_key] for i in range(0, len(selected_tmp_sub), num_random_per_key)]
        
        tmp_sub = []
        for sub in sublists:
            tmp = []
            for s in sub:
                # è¿‡æ»¤æ¡ä»¶ï¼š
                # 1. ç¬¦åˆæ ‡è¯†ç¬¦æ ¼å¼
                # 2. ä¸åœ¨åŸå§‹ä»£ç ä¸­å‡ºç°
                if bool(uid_pattern.match(s)) and code.find(s) == -1:
                    tmp.append(s)
            tmp_sub.append(tmp)
        
        # åˆ›å»ºæ–°çš„æ›¿æ¢è¯å­—å…¸
        sampled_substitutes = dict(zip(substitutes.keys(), tmp_sub))
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_sampled = sum(len(v) for v in sampled_substitutes.values())
        logger.info(f"âœ“ é‡‡æ ·å®Œæˆ")
        logger.info(f"  åŸå§‹å˜é‡æ•°: {len(substitutes)}")
        logger.info(f"  é‡‡æ ·åçš„æ›¿æ¢è¯æ€»æ•°: {total_sampled}")
        logger.debug(f"  ç¤ºä¾‹: {dict(list(sampled_substitutes.items())[:2])}")
        
        return sampled_substitutes
    
    def generate_substitutes_with_algorithm(self, code1, code2, language='java', block_size=512, top_k=60, base_model='microsoft/codebert-base-mlm', model_id=None, model_name=None, **kwargs):
        """
        ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯ï¼ˆåŸºäºCodeBERT MLMï¼‰
        
        Args:
            code1: ä»£ç 1
            code2: ä»£ç 2
            language: ç¼–ç¨‹è¯­è¨€
            block_size: ä»£ç å—å¤§å°
            top_k: æ¯ä½ç½®å€™é€‰è¯æ•°é‡
            base_model: é»˜è®¤MLMæ¨¡å‹è·¯å¾„
            model_id: æ¨¡å‹IDï¼ˆå¯é€‰ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            æ›¿ä»£è¯å­—å…¸ {identifier: [candidates]}
        
        ç®—æ³•æµç¨‹ï¼ˆå‚è€ƒget_substitutes.pyï¼‰:
        1. æå–ä»£ç æ ‡è¯†ç¬¦
        2. ä½¿ç”¨CodeBERT MLMé¢„æµ‹top-kå€™é€‰è¯
        3. ä½¿ç”¨cosine similarityç­›é€‰æœ€ç›¸ä¼¼çš„å€™é€‰è¯
        4. è½¬æ¢ä¸ºå®é™…è¯å¹¶éªŒè¯
        """
        import copy
        
        # æ³¨æ„ï¼šæ­¤å‡½æ•°æ²¡æœ‰æ˜¾å¼è®¾ç½®éšæœºç§å­ï¼Œå› ä¸ºMLMé¢„æµ‹æœ¬èº«æ˜¯ç¡®å®šæ€§çš„
        # ä½†ä¸get_substitutes.pyä¿æŒä¸€è‡´ï¼Œé¿å…å…¶ä»–æ½œåœ¨çš„éç¡®å®šæ€§æ“ä½œ
        from python_parser.run_parser import get_identifiers, remove_comments_and_docstrings
        from utils import (
            _tokenize, 
            get_identifier_posistions_from_code,
            get_substitues,
            is_valid_variable_name,
            is_valid_substitue
        )
        
        logger.info("ğŸ”§ å¼€å§‹ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯...")
        
        try:
            # åŠ è½½MLMæ¨¡å‹ï¼ˆæ”¯æŒé€šè¿‡model_idæˆ–model_nameï¼‰
            mlm_model, tokenizer_mlm = self._load_mlm_model(
                base_model, 
                model_id=model_id,
                model_name=model_name
            )
            device = next(mlm_model.parameters()).device
            
            # æ­¥éª¤1: æå–æ ‡è¯†ç¬¦
            try:
                identifiers, code_tokens = get_identifiers(
                    remove_comments_and_docstrings(code1, language),
                    language
                )
            except:
                identifiers, code_tokens = get_identifiers(code1, language)
            
            processed_code = " ".join(code_tokens)
            
            # æ­¥éª¤2: Tokenize
            words, sub_words, keys = _tokenize(processed_code, tokenizer_mlm)
            
            # æ­¥éª¤3: æå–æœ‰æ•ˆçš„å˜é‡å
            variable_names = []
            for name in identifiers:
                if ' ' in name[0].strip():
                    continue
                variable_names.append(name[0])
            
            logger.info(f"âœ“ æå–åˆ° {len(variable_names)} ä¸ªå˜é‡å")
            
            # æ­¥éª¤4: å‡†å¤‡è¾“å…¥
            sub_words = [tokenizer_mlm.cls_token] + sub_words[:block_size - 2] + [tokenizer_mlm.sep_token]
            input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])
            input_ids_ = input_ids_.to(device)
            
            # æ­¥éª¤5: MLMé¢„æµ‹
            logger.info("ğŸ¤– ä½¿ç”¨MLMæ¨¡å‹é¢„æµ‹å€™é€‰è¯...")
            logger.info(f"  è¾“å…¥åºåˆ—é•¿åº¦: {len(sub_words)}")
            with torch.no_grad():
                word_predictions = mlm_model(input_ids_)[0].squeeze()  # seq-len(sub) vocab
                word_pred_scores_all, word_predictions = torch.topk(word_predictions, top_k, -1)  # seq-len k
            
            word_predictions = word_predictions[1:len(sub_words) + 1, :]
            word_pred_scores_all = word_pred_scores_all[1:len(sub_words) + 1, :]
            logger.info(f"âœ“ MLMé¢„æµ‹å®Œæˆï¼Œå€™é€‰è¯å½¢çŠ¶: {word_predictions.shape}")
            
            # æ­¥éª¤6: è·å–æ ‡è¯†ç¬¦ä½ç½®
            names_positions_dict = get_identifier_posistions_from_code(words, variable_names)
            logger.info(f"âœ“ è·å–åˆ° {len(names_positions_dict)} ä¸ªæ ‡è¯†ç¬¦çš„ä½ç½®ä¿¡æ¯")
            
            # æ­¥éª¤7: ä¸ºæ¯ä¸ªæ ‡è¯†ç¬¦ç”Ÿæˆæ›¿ä»£è¯
            variable_substitue_dict = {}
            
            logger.info("ğŸ” è®¡ç®—åŸå§‹embeddings...")
            with torch.no_grad():
                orig_embeddings = mlm_model.roberta(input_ids_)[0]
            logger.info("âœ“ åŸå§‹embeddingsè®¡ç®—å®Œæˆ")
            
            cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
            
            total_vars = len(names_positions_dict)
            processed_vars = 0
            start_time_loop = time.time()
            for tgt_word in names_positions_dict.keys():
                processed_vars += 1
                tgt_positions = names_positions_dict[tgt_word]
                
                if not is_valid_variable_name(tgt_word, lang=language):
                    logger.debug(f"  è·³è¿‡å˜é‡ {processed_vars}/{total_vars}: {tgt_word} (æ— æ•ˆå˜é‡å)")
                    continue
                
                logger.info(f"  å¤„ç†å˜é‡ {processed_vars}/{total_vars}: {tgt_word} (å…± {len(tgt_positions)} ä¸ªä½ç½®)")
                
                # æ”¶é›†æ‰€æœ‰ä½ç½®çš„æ›¿ä»£è¯
                all_substitues = []
                
                for pos_idx, one_pos in enumerate(tgt_positions):
                    logger.debug(f"    å¤„ç†ä½ç½® {pos_idx+1}/{len(tgt_positions)}: {one_pos}")
                    if keys[one_pos][0] >= word_predictions.size()[0]:
                        continue
                    
                    substitutes = word_predictions[keys[one_pos][0]:keys[one_pos][1]]  # L, k
                    word_pred_scores = word_pred_scores_all[keys[one_pos][0]:keys[one_pos][1]]
                    
                    # ç¡®ä¿ substitutes åœ¨ device/id ä¸Šä¸ input_ids_ ä¸€è‡´ï¼ˆé˜²æ­¢è®¾å¤‡ä¸åŒ¹é…ï¼‰
                    # æ³¨æ„ï¼šword_predictions åº”è¯¥å·²åœ¨ device ä¸Šï¼Œä½†ä¿é™©èµ·è§åŠ æ­¤æ£€æŸ¥
                    if substitutes.device != device:
                        logger.warning(f"è®¾å¤‡ä¸åŒ¹é…: substitutes åœ¨ {substitutes.device}, device æ˜¯ {device}")
                        substitutes = substitutes.to(device)
                        word_pred_scores = word_pred_scores.to(device)
                    
                    orig_word_embed = orig_embeddings[0][keys[one_pos][0]+1:keys[one_pos][1]+1].to(device)
                    
                    # ä½¿ç”¨cosine similarityç­›é€‰
                    similar_substitutes = []
                    similar_word_pred_scores = []
                    sims = []
                    subwords_leng, nums_candis = substitutes.size()
                    logger.info(f"    ä½ç½® {one_pos}: éœ€è¦è®¡ç®— {nums_candis} ä¸ªå€™é€‰è¯çš„ç›¸ä¼¼åº¦ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...ï¼‰")
                    
                    # æ‰¹é‡å¤„ç†å€™é€‰è¯ï¼Œæ¯æ‰¹å¤„ç†ä¸€éƒ¨åˆ†ä»¥å‡å°‘æ—¥å¿—è¾“å‡º
                    batch_size = max(10, nums_candis // 10)  # æ¯æ‰¹è‡³å°‘10ä¸ªï¼Œæˆ–æ€»æ•°çš„10%
                    pos_start_time = time.time()
                    for batch_start in range(0, nums_candis, batch_size):
                        batch_end = min(batch_start + batch_size, nums_candis)
                        if batch_start == 0 or (batch_start + batch_size) % (batch_size * 5) == 0:
                            elapsed = time.time() - pos_start_time
                            logger.info(f"    å¤„ç†å€™é€‰è¯ {batch_start+1}-{batch_end}/{nums_candis} (å·²ç”¨æ—¶: {elapsed:.1f}ç§’)")
                        
                        for i in range(batch_start, batch_end):
                            new_ids_ = copy.deepcopy(input_ids_)
                            # ç¡®ä¿ new_ids_ åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            if new_ids_.device != device:
                                new_ids_ = new_ids_.to(device)
                            # æ›¿æ¢è¯å¾—åˆ°æ–°embeddings
                            new_ids_[0][keys[one_pos][0]+1:keys[one_pos][1]+1] = substitutes[:, i]
                            
                            with torch.no_grad():
                                new_embeddings = mlm_model.roberta(new_ids_)[0]
                            new_word_embed = new_embeddings[0][keys[one_pos][0]+1:keys[one_pos][1]+1]
                            
                            sim = sum(cos(orig_word_embed, new_word_embed)) / subwords_leng
                            sims.append((i, sim.item()))
                    
                    pos_elapsed = time.time() - pos_start_time
                    logger.info(f"    âœ“ ä½ç½® {one_pos} å¤„ç†å®Œæˆï¼Œç”¨æ—¶: {pos_elapsed:.1f}ç§’")
                    
                    # æ’åºå–top 30
                    sims = sorted(sims, key=lambda x: x[1], reverse=True)
                    
                    for i in range(int(nums_candis / 2)):
                        similar_substitutes.append(substitutes[:, sims[i][0]].reshape(subwords_leng, -1))
                        similar_word_pred_scores.append(word_pred_scores[:, sims[i][0]].reshape(subwords_leng, -1))
                    
                    if len(similar_substitutes) == 0:
                        continue
                        
                    similar_substitutes = torch.cat(similar_substitutes, 1).to(device)
                    similar_word_pred_scores = torch.cat(similar_word_pred_scores, 1).to(device)
                    
                    # è½¬æ¢ä¸ºå®é™…è¯
                    substitutes = get_substitues(
                        similar_substitutes,
                        tokenizer_mlm,
                        mlm_model,
                        1,  # use_bpe
                        similar_word_pred_scores,
                        0   # threshold
                    )
                    all_substitues += substitutes
                
                all_substitues = set(all_substitues)
                
                # éªŒè¯å¹¶æ·»åŠ æ›¿ä»£è¯
                for tmp_substitue in all_substitues:
                    if tmp_substitue.strip() in variable_names:
                        continue
                    if not is_valid_substitue(tmp_substitue.strip(), tgt_word, language):
                        continue
                    if tgt_word not in variable_substitue_dict:
                        variable_substitue_dict[tgt_word] = []
                    variable_substitue_dict[tgt_word].append(tmp_substitue)
                
                var_elapsed = time.time() - start_time_loop
                logger.info(f"  âœ“ å˜é‡ {tgt_word} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(variable_substitue_dict.get(tgt_word, []))} ä¸ªæ›¿ä»£è¯ (æ€»ç”¨æ—¶: {var_elapsed:.1f}ç§’)")
            
            total_elapsed = time.time() - start_time_loop
            logger.info(f"âœ“ æˆåŠŸç”Ÿæˆæ›¿ä»£è¯ï¼ŒåŒ…å« {len(variable_substitue_dict)} ä¸ªæ ‡è¯†ç¬¦ (æ€»ç”¨æ—¶: {total_elapsed:.1f}ç§’)")
            for var, subs in list(variable_substitue_dict.items())[:3]:
                logger.debug(f"  {var}: {len(subs)} ä¸ªå€™é€‰è¯")
            
            return variable_substitue_dict
            
        except Exception as e:
            logger.error(f"âœ— ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯å¤±è´¥: {e}", exc_info=True)
            return {}
    
    def load_substitutes_from_file(self, file_path=None):
        """
        ä»æ–‡ä»¶åŠ è½½æ›¿ä»£è¯
        
        Args:
            file_path: æ›¿ä»£è¯æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºdataset/preprocess/test_subs_clone.jsonl
        
        Returns:
            æ›¿ä»£è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«substituteså­—æ®µçš„å­—å…¸
        """
        if file_path is None:
            # é»˜è®¤è·¯å¾„
            file_path = BASE_DIR / 'dataset' / 'preprocess' / 'test_subs_clone.jsonl'
        
        substitutes_list = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    data = json.loads(line)
                    substitutes_list.append(data.get('substitutes', {}))
            
            logger.info(f"âœ“ ä»æ–‡ä»¶åŠ è½½äº† {len(substitutes_list)} ä¸ªæ ·æœ¬çš„æ›¿ä»£è¯: {file_path}")
            return substitutes_list
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ›¿ä»£è¯æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def get_substitutes_for_code(self, code_data, strategy='a', **kwargs):
        """
        è·å–ä»£ç çš„æ›¿ä»£è¯ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            code_data: åŒ…å«code1å’Œcode2çš„å­—å…¸
            strategy: è·å–ç­–ç•¥ ('file' æˆ– 'algorithm')
            **kwargs: å…¶ä»–å‚æ•°
                - file_index: æ–‡ä»¶ä¸­çš„ç´¢å¼•ï¼ˆå½“strategy='file'æ—¶ï¼‰
                - language: ç¼–ç¨‹è¯­è¨€ï¼ˆå½“strategy='algorithm'æ—¶ï¼‰
        
        Returns:
            æ›¿ä»£è¯å­—å…¸
        """
        # if strategy == 'file':
        #     # ä»æ–‡ä»¶åŠ è½½
        #     substitutes_list = self.load_substitutes_from_file()
        #     file_index = kwargs.get('file_index', 0)
            
        #     if 0 <= file_index < len(substitutes_list):
        #         return substitutes_list[file_index]
        #     elif len(substitutes_list) > 0:
        #         logger.warning("âš ï¸ æœªæŒ‡å®šfile_indexï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ›¿ä»£è¯")
        #         return substitutes_list[0]
        #     else:
        #         logger.error("âœ— æ–‡ä»¶ä¸­æ²¡æœ‰æ›¿ä»£è¯")
        #         return {}
        if strategy == 'algorithm':
            # ä½¿ç”¨ç®—æ³•ç”Ÿæˆ
            code1 = code_data.get('code1')
            code2 = code_data.get('code2', '')
            language = kwargs.get('language', 'java')
            
            return self.generate_substitutes_with_algorithm(code1, code2, language)
        else:
            logger.error(f"âœ— æœªçŸ¥çš„è·å–ç­–ç•¥: {strategy}")
            return {}
    
    def attack(self, code_data: Dict[str, str], target_model='codebert', language='java', config=None, method='itgen'):
        """
        æ‰§è¡Œå•ç»„æ•°æ®æ”»å‡» - ä½¿ç”¨ç»Ÿä¸€æ”»å‡»æ¥å£

        Args:
            code_data: åŒ…å«code1å’Œcode2çš„å­—å…¸
            target_model: ç›®æ ‡æ¨¡å‹åç§°
            language: ç¼–ç¨‹è¯­è¨€
            config: æ”»å‡»é…ç½®å‚æ•°
            method: æ”»å‡»æ–¹æ³• ('itgen', 'beam', 'alert', etc.)

        Returns:
            æ”»å‡»ç»“æœå­—å…¸
        """
        start_time = time.time()
        logger.info("=" * 60)
        logger.info("ğŸ¯ å¼€å§‹å•æ¬¡æ”»å‡»ä»»åŠ¡")
        logger.info(f"æ¨¡å‹: {target_model}, æ–¹æ³•: {method}, è¯­è¨€: {language}")
        logger.info("=" * 60)

        try:
            # ========== æ­¥éª¤1: éªŒè¯è¾“å…¥æ•°æ® ==========
            logger.info("ğŸ“ æ­¥éª¤1: éªŒè¯è¾“å…¥æ•°æ®")
            code1 = code_data.get('code1', '').strip()
            code2 = code_data.get('code2', '').strip()

            if not code1:
                raise ValueError("code1ä¸èƒ½ä¸ºç©º")

            logger.info(f"âœ“ ä»£ç 1é•¿åº¦: {len(code1)} å­—ç¬¦")
            if code2:
                logger.info(f"âœ“ ä»£ç 2é•¿åº¦: {len(code2)} å­—ç¬¦")

            # éªŒè¯é…ç½®å‚æ•°
            if config is None:
                config = {}
            true_label = config.get('true_label', 1)

            logger.info(f"âœ“ çœŸå®æ ‡ç­¾: {true_label}")
            logger.info(f"âœ“ æ”»å‡»æ–¹æ³•: {method}")

            # ========== æ­¥éª¤2: åŠ è½½æ¨¡å‹å’Œåˆ›å»ºç»Ÿä¸€æ”»å‡»å™¨ ==========
            logger.info("\nğŸ“¦ æ­¥éª¤2: åŠ è½½æ¨¡å‹å’Œæ”»å‡»å™¨")
            model_id = config.get('model_id')
            task_type = config.get('task_type', 'clone-detection')

            # åŠ è½½æ¨¡å‹
            model_data = self._load_model(target_model, model_id=model_id, task_type=task_type)
            model = model_data['model']
            tokenizer = model_data['tokenizer']

            # åˆ›å»ºç»Ÿä¸€æ”»å‡»å™¨
            attacker = self._create_unified_attacker(method, model, tokenizer, config)
            logger.info("âœ“ æ¨¡å‹å’Œæ”»å‡»å™¨å‡†å¤‡å°±ç»ª")

            # ========== æ­¥éª¤3: å‡†å¤‡æ›¿ä»£è¯ ==========
            logger.info("\nğŸ”¤ æ­¥éª¤3: å‡†å¤‡æ›¿ä»£è¯")

            if 'substitutes' in config and config['substitutes']:
                substitutes = config['substitutes']
                logger.info(f"âœ“ ä½¿ç”¨å¤–éƒ¨æä¾›çš„æ›¿ä»£è¯ï¼ŒåŒ…å« {len(substitutes)} ä¸ªæ ‡è¯†ç¬¦")
                for identifier, candidates in list(substitutes.items())[:3]:
                    logger.debug(f"  - {identifier}: {len(candidates)} ä¸ªå€™é€‰è¯")
            else:
                logger.warning("âš  æœªæä¾›æ›¿ä»£è¯ï¼Œå°è¯•ç”Ÿæˆæ›¿ä»£è¯")
                # å°è¯•ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯
                try:
                    substitutes = self.generate_substitutes_with_algorithm(
                        code1, code2, language=language, model_id=model_id, model_name=target_model
                    )
                    if substitutes:
                        logger.info(f"âœ“ ä½¿ç”¨ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯ï¼ŒåŒ…å« {len(substitutes)} ä¸ªæ ‡è¯†ç¬¦")
                    else:
                        logger.warning("âš  ç®—æ³•ç”Ÿæˆæ›¿ä»£è¯å¤±è´¥")
                        substitutes = {}
                except Exception as e:
                    logger.warning(f"âš  ç”Ÿæˆæ›¿ä»£è¯å¤±è´¥: {e}")
                    substitutes = {}

            if len(substitutes) == 0:
                logger.warning("âš  æ›¿ä»£è¯ä¸ºç©º")
                return {
                    'success': False,
                    'original_code': code1,
                    'adversarial_code': None,
                    'replaced_identifiers': None,
                    'query_times': 0,
                    'time_cost': round((time.time() - start_time) / 60, 2),
                    'error': 'æ›¿ä»£è¯ä¸ºç©º'
                }

            # ========== æ­¥éª¤4: æ‰§è¡Œç»Ÿä¸€æ”»å‡» ==========
            logger.info(f"\nâš”ï¸ æ­¥éª¤4: æ‰§è¡Œ{method.upper()}æ”»å‡»")

            # ä½¿ç”¨ç»Ÿä¸€æ”»å‡»æ¥å£
            result = attacker.attack(
                code_data=code_data,
                true_label=true_label,
                substitutes=substitutes
            )

            # æ›´æ–°æ—¶é—´æˆæœ¬
            result['time_cost'] = round((time.time() - start_time) / 60, 2)

            if result['success']:
                logger.info("ğŸ‰ æ”»å‡»æˆåŠŸï¼ç”Ÿæˆäº†æœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬")
                logger.info(f"æŸ¥è¯¢æ¬¡æ•°: {result['query_times']}")
                logger.info(f"è€—æ—¶: {result['time_cost']:.2f} åˆ†é’Ÿ")

                if result['replaced_identifiers']:
                    logger.info(f"æ›¿æ¢äº† {len(result['replaced_identifiers'])} ä¸ªæ ‡è¯†ç¬¦:")
                    for old, new in list(result['replaced_identifiers'].items())[:3]:
                        logger.info(f"  - {old} â†’ {new}")
            else:
                logger.warning("âš  æ”»å‡»å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬")
                logger.warning(f"æŸ¥è¯¢æ¬¡æ•°: {result['query_times']}")
                logger.warning(f"è€—æ—¶: {result['time_cost']:.2f} åˆ†é’Ÿ")
                if result['error']:
                    logger.warning(f"é”™è¯¯ä¿¡æ¯: {result['error']}")

            return result

        except Exception as e:
            logger.error(f"\nâœ— æ”»å‡»å¤±è´¥: {str(e)}", exc_info=True)

            # è¿”å›é”™è¯¯ç»“æœ
            return {
                'success': False,
                'original_code': code_data.get('code1', ''),
                'adversarial_code': None,
                'replaced_identifiers': None,
                'query_times': 0,
                'time_cost': round((time.time() - start_time) / 60, 2),
                'error': str(e)
            }
        finally:
            logger.info("\n" + "=" * 60)
            logger.info("âœ“ æ”»å‡»ä»»åŠ¡ç»“æŸ")
            logger.info("=" * 60)
    
# execute_script_attack æ–¹æ³•å·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨ attack() æ–¹æ³•
 
